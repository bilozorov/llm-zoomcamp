{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Open-Source LLMs\n",
    "## Homework\n",
    "\n",
    "In this homework, we'll experiment more with Ollama. Original description of tasks is [here](https://github.com/DataTalksClub/llm-zoomcamp/blob/main/cohorts/2024/02-open-source/homework.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Running Ollama with Docker\n",
    "\n",
    "Let's run ollama with Docker. We will need to execute the \n",
    "same command as in the lectures:\n",
    "\n",
    "```bash\n",
    "docker run -it \\\n",
    "    --rm \\\n",
    "    -v ollama:/root/.ollama \\\n",
    "    -p 11434:11434 \\\n",
    "    --name ollama \\\n",
    "    ollama/ollama\n",
    "```\n",
    "\n",
    "What's the version of ollama client? \n",
    "\n",
    "To find out, enter the container and execute `ollama` with the `-v` flag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "docker run -it --rm -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "docker ps\n",
    "docker exec -it ollama bash\n",
    "ollama -v\n",
    "exit\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer for Q1\n",
    "ollama version is 0.1.48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Downloading an LLM \n",
    "\n",
    "We will donwload a smaller LLM - gemma:2b. \n",
    "\n",
    "Again let's enter the container and pull the model:\n",
    "\n",
    "```bash\n",
    "ollama pull gemma:2b\n",
    "```\n",
    "\n",
    "In docker, it saved the results into `/root/.ollama`\n",
    "\n",
    "We're interested in the metadata about this model. You can find\n",
    "it in `models/manifests/registry.ollama.ai/library`\n",
    "\n",
    "What's the content of the file related to gemma?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "docker exec -it ollama bash\n",
    "ollama pull gemma:2b\n",
    "cat /root/.ollama/models/manifests/registry.ollama.ai/library/gemma/2b\n",
    "\n",
    "# or after all:\n",
    "docker exec ollama cat /root/.ollama/models/manifests/registry.ollama.ai/library/gemma/2b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer for Q2\n",
    "\n",
    "{\"schemaVersion\":2,\"mediaType\":\"application/vnd.docker.distribution.manifest.v2+json\",\"config\":{\"mediaType\":\"application/vnd.docker.container.image.v1+json\",\"digest\":\"sha256:887433b89a901c156f7e6944442f3c9e57f3c55d6ed52042cbb7303aea994290\",\"size\":483},\"layers\":[{\"mediaType\":\"application/vnd.ollama.image.model\",\"digest\":\"sha256:c1864a5eb19305c40519da12cc543519e48a0697ecd30e15d5ac228644957d12\",\"size\":1678447520},{\"mediaType\":\"application/vnd.ollama.image.license\",\"digest\":\"sha256:097a36493f718248845233af1d3fefe7a303f864fae13bc31a3a9704229378ca\",\"size\":8433},{\"mediaType\":\"application/vnd.ollama.image.template\",\"digest\":\"sha256:109037bec39c0becc8221222ae23557559bc594290945a2c4221ab4f303b8871\",\"size\":136},{\"mediaType\":\"application/vnd.ollama.image.params\",\"digest\":\"sha256:22a838ceb7fb22755a3b0ae9b4eadde629d19be1f651f73efb8c6b4e2cd0eea0\",\"size\":84}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Running the LLM\n",
    "\n",
    "Test the following prompt: \"10 * 10\". What's the answer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First way:\n",
    "```bash\n",
    "docker exec -it ollama bash\n",
    "ollama run gemma:2b\n",
    "10*10\n",
    "```\n",
    "\n",
    "#### Second way:\n",
    "```bash\n",
    "echo \"10*10\" | docker exec -it ollama ollama run gemma:2b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer for Q3\n",
    "```bash\n",
    "Sure, here is the response to the prompt:\n",
    "\n",
    "10 * 10 = 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Donwloading the weights \n",
    "\n",
    "We don't want to pull the weights every time we run\n",
    "a docker container. Let's do it once and have them available\n",
    "every time we start a container.\n",
    "\n",
    "First, we will need to change how we run the container.\n",
    "\n",
    "Instead of mapping the `/root/.ollama` folder to a named volume,\n",
    "let's map it to a local directory:\n",
    "\n",
    "```bash\n",
    "mkdir ollama_files\n",
    "\n",
    "docker run -it \\\n",
    "    --rm \\\n",
    "    -v ./ollama_files:/root/.ollama \\\n",
    "    -p 11434:11434 \\\n",
    "    --name ollama \\\n",
    "    ollama/ollama\n",
    "```\n",
    "\n",
    "Now pull the model:\n",
    "\n",
    "```bash\n",
    "docker exec -it ollama ollama pull gemma:2b \n",
    "```\n",
    "\n",
    "What's the size of the `ollama_files/models` folder? \n",
    "\n",
    "* 0.6G\n",
    "* 1.2G\n",
    "* 1.7G\n",
    "* 2.2G\n",
    "\n",
    "Hint: on linux, you can use `du -h` for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# docker stop ollama\n",
    "mkdir ollama_files\n",
    "docker run -it --rm -v ./ollama_files:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "docker exec -it ollama ollama pull gemma:2b\n",
    "du -h ollama_files/models\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer for Q4\n",
    "\n",
    "1.6G\tollama_files/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Adding the weights \n",
    "\n",
    "Let's now stop the container and add the weights \n",
    "to a new image\n",
    "\n",
    "For that, let's create a `Dockerfile`:\n",
    "\n",
    "```dockerfile\n",
    "FROM ollama/ollama\n",
    "\n",
    "COPY ...\n",
    "```\n",
    "\n",
    "What do you put after `COPY`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
